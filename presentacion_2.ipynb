{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentación 2, Redes Neuronales Avanzadas: Stable Diffusion.\n",
    "\n",
    "Cristian Pinoleo\n",
    "\n",
    "Oscar Painen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a los modelos de Difusión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos Generativos.\n",
    "\n",
    "Un modelo generativo aprende la distribución de probabilidad del conjunto de datos tal que podemos obtener una muestra de esta distribución para generar nuevas instancias de los datos. En general, esta probabilidad no es univariada, por lo tanto la distribución a aprender es conjunta.\n",
    "\n",
    "Por ejemplo, si entrenamos un modelo generativo con muchas imágenes de gatos, entonces podemos sacar una muestra de la distribución aprendida para generar más imágenes de gatos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos generativos\\generative_vs_discriminative_alt.jpg\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Función de modelos generativos vs modelos discriminativos. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momentos hemos explorado los Generative Adversarial Networks (GANs) y Variational Autoencoders (VAEs). Sin embargo, no son los únicos modelos generativos para imágenes, sino que existen una variedad de modelos disponibles para esta misma tarea."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos generativos\\generative_ai_timeline.png\" style=\"height:50%; width:60%;\">\n",
    "<figcaption align = \"center\"> Línea de tiempo de distintos modelos generativos. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Difusión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos generativos\\generative_models_comparative_graphic.png\" style=\"height:50%; width:60%;\">\n",
    "<figcaption align = \"center\"> Comparación de los resultados de distintos modelos generativos. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de difusión contienen dos pasos esenciales, forward y reverse denoising.\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos de difusion\\diffusion_processes.PNG\" style=\"height:50%; width:100%;\">\n",
    "<figcaption align = \"center\"> Ejemplo del proceso de difusión. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos de difusion\\different_generative_models.png\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Comparación de la estructura de distintos modelos generativos. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un punto muestreado de una distribución $x_0\\sim q(x)$ de los datos, definimos el *proceso de difusión hacia adelante* en donde agregamos ruido Gaussiano a la muestra en $T$ pasos, produciendo una secuencia de muestras ruidosas $x_1, x_2,\\dots,x_T$. El paso con el cual agregamos el ruido está dado por $\\{\\beta_t\\in(0,1)\\}_{t=1}^{T}$, estos términos también corresponden a la varianza de la Gaussiana. Así, nuestra probabilidad condicional para movernos hacia adelante es:\n",
    "$$\n",
    "    q(x_t|x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\;\\beta_t\\mathbb{I}),\\quad q(x_{1:T}|x_0) = \\Pi_{t=1}^{T}q(x_t|x_{t-1})\n",
    "$$\n",
    "\n",
    "En este sentido, debemos aplicar el ruido de manera secuencial, sin embargo, dado que nos encontramos tratando con ruido Gaussiano cadenas de Markov, podemos aplicar el famoso *truco de reparametrización* para poder obtener las imágenes ruidosas en cualquier tiempo $t$ sin la necesidad de tener todas las imágenes. De esta forma, sea $\\alpha_t = 1 - \\beta_t$ y $\\overline{\\alpha}_t = \\Pi_{i=1}^{t}\\alpha_i$, entonces:\n",
    "$$\n",
    "    q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline{\\alpha}_t}x_0,\\; (1-\\overline{\\alpha}_t)\\mathbb{I})\n",
    "$$\n",
    "\n",
    "Notamos que en el proceso anterior no es necesario entrenar nada, simplemente puede ser aplicado directamente a una imagen cualquiera. Sin embargo, necesitamos revertir dicho proceso, aquí entran las redes neuronales. Idealmente podríamos utilizar $q(x_{t-1}|x_t)$ para volver a las imágenes originales. Desafortunadamente, no podemos aproximar $q(x_{t-1}|x_t)$ tan fácilmente. Por lo que entrenamos a la red para aprender una aproximación $p_{\\theta}$, y así poder revertir el proceso. La probabilidad aproximada tiene la siguiente forma:\n",
    "$$\n",
    "    p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_t,t),\\;\\Sigma_{\\theta}(x_t,t))\n",
    "$$\n",
    "Cabe destacar que la probabilidad condicional $q$ es posible de calcular cuando está condicionada en $x_0$ \n",
    "$$\n",
    "    q(x_{t-1}|x_t,x_0) = \\mathcal{N}(x_{t-1};\\tilde{\\mu}(x_t,x_0),\\;\\tilde{\\beta}_t\\mathbb{I})\n",
    "$$\n",
    "Donde:\n",
    "$$\n",
    "    \\tilde{\\beta}_t = \\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_t}\\cdot \\beta_t,\\quad \\tilde{\\mu}_t(x_t,x_0) = \\frac{\\sqrt{\\alpha_t}(1-\\overline{\\alpha}_{t-1})}{1-\\overline{\\alpha}_t}x_t + \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}\\beta_t}{1-\\overline{\\alpha}_t}x_0\n",
    "$$\n",
    "Por último, notamos que podemos representar $x_0 = \\frac{1}{\\sqrt{\\overline{\\alpha}_t}}(x_t - \\sqrt{1-\\overline{\\alpha}_t}\\epsilon_t)$ con $\\epsilon_t \\sim \\mathcal{N}(0,\\mathbb{I})$, lo que nos permite escribir la media como:\n",
    "$$\n",
    "    \\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_t\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos de difusion\\DDPM.png\" style=\"height:50%; width:100%;\">\n",
    "<figcaption align = \"center\"> Ejemplo del proceso de difusión. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos de difusion\\diffusion_process_animation.gif\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Ejemplo del proceso de difusión. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes\\modelos difusion\\modelos de difusion\\nonequilibrium_thermodynamics_results.PNG\" style=\"height:50%; width:100%;\">\n",
    "<figcaption align = \"center\"> Resultados del primer modelo de difusión sobre la base de datos CIFAR-10. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos que una de nuestras metas es aproximar la media $\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_t\\right)$ para el proceso de difusión hacia atrás $p_\\theta = \\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\;\\Sigma_\\theta(x_t,t))$. Podemos reparametrizar el término de ruido Gaussiano, así haciendo que la predicción sea $\\epsilon_t$ para una imagen $x_t$ (en un paso t). Lo que resulta en:\n",
    "$$\n",
    "    \\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right)\n",
    "$$\n",
    "\n",
    "Gracias a esto, podemos establecer la función de pérdida, calculando el error cuadrático medio, de forma [simple](https://arxiv.org/pdf/2006.11239.pdf):\n",
    "$$\n",
    "    L_\\text{simple}(\\theta) := \\mathbb{E}_{x_0\\sim q(x_0), \\epsilon\\sim\\mathcal{N}(0,\\mathbb{I}), t \\sim \\mathcal{U}(1, T)}\\left[ \\|\\epsilon - \\epsilon_\\theta(x_t,t)) \\|^2 \\right] = \\mathbb{E}_{x_0, \\epsilon, t}\\left[ \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\epsilon,t) \\|^2 \\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de difusión en espacios latentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes/stable diffusion/latent diffusion models/latent_diffusion.png\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Ejemplo del proceso de difusión en espacios latentes. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes/stable diffusion/latent diffusion models/latent_diffusion_animation.gif\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Animación del proceso de difusión en espacios latentes. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"imagenes/modelos difusion/stable diffusion/stable_diffusion_architecture.png\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Arquitectura del modelo Stable Diffusion. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el gran modelo revolucionario propuesto en el paper. El proceso de difusión se lleva a cabo en el espacio latente. Lo que provoca obtener resultados similares o mejores que otros modelos generativos con un costo computacional y energetico menor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condicionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow --upgrade\n",
    "#!pip install keras-cv --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:20:41.321328: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 13:20:41.323708: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 13:20:41.356737: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 13:20:41.356760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 13:20:41.357670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 13:20:41.363134: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 13:20:41.363318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 13:20:42.057513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "model = keras_cv.models.StableDiffusion(jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
      "1356917/1356917 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:21:28.957470: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
      "492466864/492466864 [==============================] - 21s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:21:55.941905: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7e640336a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-27 13:21:55.941951: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-27 13:21:56.054262: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1701102118.804685 2041729 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-11-27 13:21:58.809957: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding shape: (77, 768)\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "interpolation_steps = 5\n",
    "\n",
    "encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n",
    "encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n",
    "\n",
    "interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n",
    "\n",
    "# Show the size of the latent manifold\n",
    "print(f\"Encoding shape: {encoding_1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
      "3439090152/3439090152 [==============================] - 144s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:25:03.672773: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.178 = f32[5,1,1,32,2]{4,3,2,1,0} reduce-window(f32[5,32,32,32,60]{4,3,2,1,0} %broadcast.537, f32[] %constant.1019), window={size=1x32x32x1x32 stride=1x32x32x1x32 pad=0_0x0_0x0_0x0_0x2_2}, to_apply=%diffusion_model_res_block_16_group_normalization_42_weighted_moments_sum_of_weights-reduction.1021\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2023-11-27 13:25:04.016166: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.343523261s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %reduce-window.178 = f32[5,1,1,32,2]{4,3,2,1,0} reduce-window(f32[5,32,32,32,60]{4,3,2,1,0} %broadcast.537, f32[] %constant.1019), window={size=1x32x32x1x32 stride=1x32x32x1x32 pad=0_0x0_0x0_0x0_0x2_2}, to_apply=%diffusion_model_res_block_16_group_normalization_42_weighted_moments_sum_of_weights-reduction.1021\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2023-11-27 13:25:07.144899: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:\n",
      "\n",
      "  %reduce-window.214 = f32[5,2,2,32,1]{4,3,2,1,0} reduce-window(f32[5,64,64,32,30]{4,3,2,1,0} %broadcast.618, f32[] %constant.1187), window={size=1x32x32x1x30 stride=1x32x32x1x30}, to_apply=%diffusion_model_res_block_19_group_normalization_51_weighted_moments_sum_of_weights-reduction.1189\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2023-11-27 13:25:07.686088: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.545480414s\n",
      "Constant folding an instruction is taking > 2s:\n",
      "\n",
      "  %reduce-window.214 = f32[5,2,2,32,1]{4,3,2,1,0} reduce-window(f32[5,64,64,32,30]{4,3,2,1,0} %broadcast.618, f32[] %constant.1187), window={size=1x32x32x1x30 stride=1x32x32x1x30}, to_apply=%diffusion_model_res_block_19_group_normalization_51_weighted_moments_sum_of_weights-reduction.1189\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 987s 16s/step\n",
      "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n",
      "198180272/198180272 [==============================] - 9s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:39:07.888760: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 4s:\n",
      "\n",
      "  %reduce-window.35 = f32[5,4,4,32,1]{4,3,2,1,0} reduce-window(f32[5,128,128,32,16]{4,3,2,1,0} %broadcast.133, f32[] %constant.659), window={size=1x32x32x1x16 stride=1x32x32x1x16}, to_apply=%decoder_resnet_block_5_group_normalization_72_weighted_moments_sum_of_weights-reduction.661\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2023-11-27 13:39:09.114271: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 5.235697336s\n",
      "Constant folding an instruction is taking > 4s:\n",
      "\n",
      "  %reduce-window.35 = f32[5,4,4,32,1]{4,3,2,1,0} reduce-window(f32[5,128,128,32,16]{4,3,2,1,0} %broadcast.133, f32[] %constant.659), window={size=1x32x32x1x16 stride=1x32x32x1x16}, to_apply=%decoder_resnet_block_5_group_normalization_72_weighted_moments_sum_of_weights-reduction.661\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "seed = 12345\n",
    "noise = tf.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n",
    "\n",
    "images = model.generate_image(\n",
    "    interpolated_encodings,\n",
    "    batch_size=interpolation_steps,\n",
    "    diffusion_noise=noise,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n",
    "    if rubber_band:\n",
    "        images += images[2:-1][::-1]\n",
    "    images[0].save(\n",
    "        filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=1000 // frames_per_second,\n",
    "        loop=0,\n",
    "    )\n",
    "\n",
    "\n",
    "export_as_gif(\n",
    "    \"doggo-and-fruit-5.gif\",\n",
    "    [Image.fromarray(img) for img in images],\n",
    "    frames_per_second=2,\n",
    "    rubber_band=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"doggo-and-fruit-5.gif\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Results </figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 393s 16s/step\n",
      "25/25 [==============================] - 395s 16s/step\n",
      "25/25 [==============================] - 387s 15s/step\n",
      "25/25 [==============================] - 391s 16s/step\n",
      "25/25 [==============================] - 393s 16s/step\n",
      "25/25 [==============================] - 385s 15s/step\n",
      "25/25 [==============================] - 381s 15s/step\n",
      "25/25 [==============================] - 385s 15s/step\n",
      "25/25 [==============================] - 385s 15s/step\n",
      "25/25 [==============================] - 382s 15s/step\n"
     ]
    }
   ],
   "source": [
    "interpolation_steps = 50\n",
    "batch_size = 5\n",
    "batches = interpolation_steps // batch_size\n",
    "\n",
    "interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n",
    "batched_encodings = tf.split(interpolated_encodings, batches)\n",
    "\n",
    "images = []\n",
    "for batch in range(batches):\n",
    "    images += [\n",
    "        Image.fromarray(img)\n",
    "        for img in model.generate_image(\n",
    "            batched_encodings[batch],\n",
    "            batch_size=batch_size,\n",
    "            num_steps=25,\n",
    "            diffusion_noise=noise,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "export_as_gif(\"doggo-and-fruit-150.gif\", images, rubber_band=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"doggo-and-fruit-150.gif\" style=\"height:50%; width:80%;\">\n",
    "<figcaption align = \"center\"> Results </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 284s 11s/step\n",
      "25/25 [==============================] - 284s 11s/step\n",
      "25/25 [==============================] - 281s 11s/step\n",
      "25/25 [==============================] - 282s 11s/step\n",
      "25/25 [==============================] - 280s 11s/step\n",
      "25/25 [==============================] - 282s 11s/step\n",
      "25/25 [==============================] - 282s 11s/step\n",
      "25/25 [==============================] - 283s 11s/step\n",
      "25/25 [==============================] - 276s 11s/step\n",
      "20/25 [=======================>......] - ETA: 52s "
     ]
    }
   ],
   "source": [
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "prompt_3 = \"The eiffel tower in the style of starry night\"\n",
    "prompt_4 = \"An architectural sketch of a skyscraper\"\n",
    "\n",
    "interpolation_steps = 6\n",
    "batch_size = 3\n",
    "batches = (interpolation_steps**2) // batch_size\n",
    "\n",
    "encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n",
    "encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n",
    "encoding_3 = tf.squeeze(model.encode_text(prompt_3))\n",
    "encoding_4 = tf.squeeze(model.encode_text(prompt_4))\n",
    "\n",
    "interpolated_encodings = tf.linspace(\n",
    "    tf.linspace(encoding_1, encoding_2, interpolation_steps),\n",
    "    tf.linspace(encoding_3, encoding_4, interpolation_steps),\n",
    "    interpolation_steps,\n",
    ")\n",
    "interpolated_encodings = tf.reshape(\n",
    "    interpolated_encodings, (interpolation_steps**2, 77, 768)\n",
    ")\n",
    "batched_encodings = tf.split(interpolated_encodings, batches)\n",
    "\n",
    "images = []\n",
    "for batch in range(batches):\n",
    "    images.append(\n",
    "        model.generate_image(\n",
    "            batched_encodings[batch],\n",
    "            batch_size=batch_size,\n",
    "            num_steps=25,\n",
    "            diffusion_noise=noise,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_grid(\n",
    "    images,\n",
    "    path,\n",
    "    grid_size,\n",
    "    scale=2,\n",
    "):\n",
    "    fig = plt.figure(figsize=(grid_size * scale, grid_size * scale))\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.axis(\"off\")\n",
    "    images = images.astype(int)\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            index = row * grid_size + col\n",
    "            plt.subplot(grid_size, grid_size, index + 1)\n",
    "            plt.imshow(images[index].astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "            plt.margins(x=0, y=0)\n",
    "    plt.savefig(\n",
    "        fname=path,\n",
    "        pad_inches=0,\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=False,\n",
    "        dpi=60,\n",
    "    )\n",
    "\n",
    "\n",
    "images = np.concatenate(images)\n",
    "plot_grid(images, \"4-way-interpolation.jpg\", interpolation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entornoCristianOscar",
   "language": "python",
   "name": "mi_entornocristianoscar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
